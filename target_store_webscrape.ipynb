{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb739a7-9846-4353-a8ed-cc53c5d52ad3",
   "metadata": {},
   "source": [
    "# Target Store Web Scraper\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This code is designed to scrape all Target stores in the USA. If you go thorugh the the target store directory and click on a state to view the cites with target stores, you will notice that clicking on some cities will go directly to a store page and others will open up an overlay with a list of stores. For example, if you visit the directory for Minnesota, https://www.target.com/store-locator/store-directory/minnesota, clicking on Bemidji or Duluth will take you directly to the store page but clicking on Saint Cloud will bring up a sidebar that shows two stores in which you can then click on and go to their respective store pages. While you can use the requests library to get the urls for Bemidji and Duluth from the html, the sidebar for Saint Cloud is not a part of the base url, and therefore has to be dynamically loaded. For this we use webdriver from the selenium library.\n",
    "\n",
    "Simply put, webdriver uses a web browser to navigate web pages and can be used input data such as usernames and passwords or simulate mouse inputs. For our case we just need to load webpages and call the html through webdriver, which we can then use our standard libraries to parse and scrape. \n",
    "\n",
    "For this program, I use google chrome and Chromedriver. Chromedriver can be downloaded from https://chromedriver.chromium.org/home and saved to a file directory of your choice. \n",
    "#### Note: Please make sure you have the correct version of Chromedriver for google chrome. Make a note of your file directory, you will need it for a later step. Also make sure you have all the required libraries included in the Imports section.\n",
    "\n",
    "Included is commented out code that can be uncommented to save some of the data objects for ready use in future code. For example, the first commented out code uses the pickle library to save a list of links to each state store directory to a text file. You can then unpickle the file which gives you back your list of links.\n",
    "\n",
    "\n",
    "#### Note: The code must run sequentially from imports down for everything to work.\n",
    "\n",
    "#### Note: There are several parts in the code where a sleep funciton has been implemented. This is so that we don't get blacklisted form Target's website or put any unecessary strain on the site. As is this code will take a few hours to complete. Use your judgement and discresion to change the time funcitons to speed or slow down the code.\n",
    "\n",
    "### Contents\n",
    "\n",
    "* [Imports](#imports)\n",
    "\n",
    "* [Support Functions](#sf)\n",
    "\n",
    "* [State Directory](#std)\n",
    "\n",
    "* [City Directory](#cd)\n",
    "\n",
    "* [Store Directory](#sd)\n",
    "\n",
    "* [Storing Object Data](#sod)\n",
    "\n",
    "* [Scraping Each Store Page](#sesp)\n",
    "\n",
    "* [Saving the Data](#stda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7d93f-0b3b-4d18-8fc5-f13e4d12a753",
   "metadata": {},
   "source": [
    "#### Imports <a class=\"anchor\" id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ec83f-6a07-410a-abcb-c7b17c9e68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Required imports\n",
    "import requests as re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8047b69-c33c-49e8-8949-dabb80d7b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will need the state abbreviations for later\n",
    "state_abbrev = {'alabama':'AL','alaska':'AK','arizona':'AZ','arkansas':'AR','california':'CA',\n",
    "               'colorado':'CO','connecticut':'CT','delaware':'DE','florida':'FL','georgia':'GA',\n",
    "               'hawaii':'HI','idaho':'ID','illinois':'IL','indiana':'IA','iowa':'IA','kansas':'KS',\n",
    "               'kentucky':'KY','louisiana':'LA','maine':'ME','maryland':'MD','massachusetts':'MA',\n",
    "               'michigan':'MI','minnesota':'MN','mississippi':'MS','missouri':'MO','montana':'MT',\n",
    "               'nebraska':'NE','nevada':'NV','new hampshire':'NH','new jersey':'NJ','new mexico':'NM',\n",
    "                'new york':'NY','north carolina':'NC','north dakota':'ND','ohio':'OH','oklahoma':'OK',\n",
    "                'oregon':'OR','pennsylvania':'PA','rhode island':'RI','south carolina':'SC','south dakota':'SD',\n",
    "                'tennessee':'TN','texas':'TX','utah':'UT','vermont':'VT','virginia':'VA','washington':'WA',\n",
    "                'west virginia':'WV','wisconsin':'WI','wyoming':'WY'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2485496-8db8-4c2a-8a82-92384c1f1563",
   "metadata": {},
   "source": [
    "#### Support Function <a class=\"anchor\" id=\"sf\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c030b72-ec6e-4b52-9110-1202f4e194b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support functions\n",
    "def states_urls(url):\n",
    "    '''\n",
    "    Gathers a list of urls for each state in target\n",
    "    store directory. \n",
    "    Creates a dictionary where the keys are the \n",
    "    state names, e.g. new-york, and the values\n",
    "    are the urls.\n",
    "    '''\n",
    "    \n",
    "    data = re.get(url).text \n",
    "    soup = bs(data,\"html.parser\")\n",
    "    # By examining the urls for particular states we need to\n",
    "    # create a list of states in lowercase to append to original url\n",
    "    # and replace spaces between states with two words with '-' so\n",
    "    # the urls will work.\n",
    "    state_dict=dict()\n",
    "    state_divs = soup.find_all('div', class_='h-margin-v-tiny')\n",
    "    for i in range(len(state_divs)):\n",
    "        state = state_divs[i].a.text.lower()\n",
    "        state_url = url + str('/') + state.replace(\" \",\"-\")\n",
    "        state_dict[state]=state_url\n",
    "        \n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def city_scrape(url):\n",
    "    '''\n",
    "    Input: url to state in target store directory\n",
    "    For a given url for a particular state, returns a list of names\n",
    "    of cites in that state that has one or more target stores\n",
    "    '''\n",
    "    state = re.get(url).text\n",
    "    soup = bs(state,'html.parser')\n",
    "    \n",
    "    cities = []\n",
    "    store_divs = soup.find_all('div', class_=\"h-margin-v-tiny\")\n",
    "    for div in store_divs:\n",
    "        if div.a==None:\n",
    "            # Cities that have multiple stores don't link to another page\n",
    "            # but instead, pull a dynamically loaded overlay with the list of stores\n",
    "            # and therefore are not enclosed in an <a> tag\n",
    "            cities.append(div.text) \n",
    "        else:\n",
    "            cities.append(div.a.text) #Those cities that have an <a> tag with a hyperlink \n",
    "    return cities\n",
    "    \n",
    "def store_scrape(url):\n",
    "    ''' \n",
    "    Given a url to a store page, this function gets the\n",
    "    store name, address, and services returns a dictionary:\n",
    "    {store_name: \"Name\", address: \"address\", services:\"list of services\"}\n",
    "    - requires import requests as re and from bs4 import BeautifulSoup as bs\n",
    "    '''\n",
    "    html = re.get(url).text\n",
    "    soup = bs(html,'html.parser')\n",
    "    store_name = soup.find('h1', class_='Heading__StyledHeading-sc-1mp23s9-0 styles__StoreNameHeading-sc-pxu7eq-0 czSHDm kqGfqM h-margin-b-tiny')\n",
    "    address = soup.find('a', rel='noopener').text\n",
    "    service_tags = soup.find_all('div', class_='styles__CollapsibleContainer-sc-983hjk-2 jmXqrN')\n",
    "    service_tags2 = soup.find_all('li', class_='styles__OtherServicesListItem-sc-7syzbd-1 hBVqPX')\n",
    "    services = []\n",
    "    for i in range(len(service_tags)):\n",
    "        services.append(service_tags[i].h3.text)\n",
    "    for i in range(len(service_tags2)):\n",
    "        services.append(service_tags2[i].text)\n",
    "    store_dict = {}\n",
    "    store_dict['store_name']=store_name.text\n",
    "    store_dict['address']=address\n",
    "    store_dict['services']=services\n",
    "    \n",
    "    \n",
    "    return store_dict  \n",
    "\n",
    "def store_search(state,cities): #state= 2 letter abbreviation,e.g. FL, cities = list of cities in state\n",
    "    '''\n",
    "    This function uses the target store_finder tool to generate a list of \n",
    "    target store urls based on a state and a list of cities in the state.\n",
    "    Because calling differnt cities in the target store finder may have overlaping target\n",
    "    stores, we include a check that excludes duplicates.\n",
    "    \n",
    "    Note: This is the step that requiers an open webdriver. Please make sure\n",
    "    you have a webdriver named driver open,\n",
    "    e.g. \n",
    "    driver = webdriver.Chrome('YOUR FILE DIRECTORY FOR CHROMEDRIVER HERE')\n",
    "    '''\n",
    "    storeurls = []\n",
    "    for i in range(len(cities)):\n",
    "        url = 'https://www.target.com/store-locator/find-stores/'+cities[i]+','+state\n",
    "        #search = re.get(url).text\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        html = driver.page_source\n",
    "        soup=bs(html,'html5lib')\n",
    "        divs=soup.find_all('div', class_='Card__CardButtons-sc-6da7hu-1 cjxhUz')\n",
    "        for div in divs:\n",
    "            storeurl = 'https://www.target.com' + div.a['href']\n",
    "            #Check if storeurl is alread in storeurls list\n",
    "            if storeurl not in storeurls:\n",
    "                storeurls.append(storeurl)\n",
    "                \n",
    "    return storeurls\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c1a432-2754-4abe-a994-a6e57b747f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_directory.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bdb326-a45f-47f3-9f1c-fbb442b040c4",
   "metadata": {},
   "source": [
    "#### State Directory <a class=\"anchor\" id=\"std\"></a>\n",
    "\n",
    "Create a dictionary where state is the key and the value is the url linking to the webpage containing the list of cities containing target stores.\n",
    "\n",
    "\n",
    "i.e state_directory = {'alabama':url, 'alaska':url, ... ,'wyoming':url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e32c7-1e45-44ae-b5e2-73cf3e18357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directory of states and their urls\n",
    "url = 'https://www.target.com/store-locator/store-directory' #url to initial target store directory containing list of states\n",
    "state_directory = states_urls(url) #Gets dictionary of the form {'state':'url',...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34175d-4046-4171-8b67-6aa231c97431",
   "metadata": {},
   "source": [
    "#### City Directory <a class=\"anchor\" id=\"cd\"></a>\n",
    "\n",
    "Create a dictionary where the values are the abbreviations of the states and the keys are lists of city names with target stores.\n",
    "\n",
    "i.e. state_cities = {'AL':['city1','city2',...], 'AK':['city1','city2',...], ... ,'WY':['city1','city2',...]}\n",
    "\n",
    "We abbreviate the state because this will be necessary in a later web scraping step.\n",
    "\n",
    "Since we are visiting a series of urls for this web scraping step we include a sleep function to wait a few seconds between each web scrape. This is so we don't unecessarily stress the website or get blacklisted. Feel free to change the sleep time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d102e066-1be6-496a-ae68-8366879c174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#For each state page get list of city names\n",
    "state_cities = dict()\n",
    "for key in state_directory:\n",
    "    time.sleep(2)\n",
    "    state_cities[state_abbrev[key]]=city_scrape(state_directory[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed686d1f-49a4-4c9c-a81b-a7812ff73160",
   "metadata": {},
   "source": [
    "#### Store Directory <a class=\"anchor\" id=\"sd\"></a>\n",
    "\n",
    "Create a list of urls for all target stores in the USA. \n",
    "\n",
    "We use the list of cities for each state in the state_cites dictionary and pass their keys and values to the store_search function.\n",
    "\n",
    "We include a check to prevent duplicates since the store_search function may pull stores from differnt states for one input.\n",
    "\n",
    "Note: The store search function includes a sleep function of 5 seconds. This is quite long but is to prevent us from overloading Target's website or getting blacklisted since we are scraping thousands of pages. Feel free to change at your discretion. See support functions above.\n",
    "\n",
    "Note: This is the step that requires the webdriver to be open. Make sure you have chromedriver installed and copy the file path into the quotations into the webdriver object below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92737874-c6e5-4768-ab74-ff5a6e347724",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('YOUR_FILE_DIRECTORY/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b179a372-bba2-482a-b723-30361d1b16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_hrefs = []#Store all the storepage links to all target stores\n",
    "for key in state_cities:\n",
    "    store_list = store_search(key,state_cities[key])\n",
    "    for store in store_list:\n",
    "        if store not in store_hrefs:\n",
    "            store_hrefs.append(store)\n",
    "    \n",
    "print(\"store page link scrape finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268ae4b-ae65-4ee1-9df1-e4a1b835b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the driver. It is not needed for further steps\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f5dc20-09a5-4997-9f9f-ff246d59ecf0",
   "metadata": {},
   "source": [
    "### Storing object data <a class=\"anchor\" id=\"sod\"></a>\n",
    "\n",
    "You might want to save the store_hrefs list for future use or incase something goes wrong in a later step. Uncomment the below cell if you wish to save the store_hrefs to a file.\n",
    "\n",
    "If you wish to recall the store_hrefs list, uncomment the second to next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb93669-43af-4de1-b7b5-147fbe6dfe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the below code if you wish to store the store_href list into a file\n",
    "\n",
    "#with open('target_store_hrefs.txt','wb') as file:\n",
    "    #pickle.dump(store_hrefs,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba2cec-2b7a-48b3-bc95-b384917af5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the below code if you wish to recall the store_hrefs list from the pickle file\n",
    "\n",
    "#pickle_off = open (\"target_store_hrefs.txt\", \"rb\")\n",
    "#store_hrefs = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f01d5e2-db8c-48b2-8034-3da8e957eb55",
   "metadata": {},
   "source": [
    "#### Checking to make sure our list of store links is unique\n",
    "\n",
    "We can check and see how many stores links we have scraped and if they are all differnt stores.\n",
    "\n",
    "The len fucntion tells you how many elements you have in your list and the set fucntion will only consider non duplicate elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba8650-9d31-4577-aa0e-4ef360a53d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of links in store_hrefs list: \", len(store_hrefs))\n",
    "print(\"Number of unique links in store_hrefs list: \", len(set(store_hrefs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05863f9b-808f-4e40-82be-bfbbd01c3beb",
   "metadata": {},
   "source": [
    "#### Scraping Each Store Page <a class=\"anchor\" id=\"sesp\"></a>\n",
    "\n",
    "We use the store_scrape support function to generate a list of dictionaries, with each dictionary corresponding to a target store and its data.\n",
    "\n",
    "Each store dictionary consist of {'store_name':'...', 'address':'...', 'services':'...'}\n",
    "\n",
    "We again include a sleep function to wait between scrapes. Change the time at your discretion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354ce94-bef7-403f-98ab-3b4927e9b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = []\n",
    "for link in store_hrefs:\n",
    "    store_data=store_scrape(link)\n",
    "    target_data.append(store_data)\n",
    "    time.sleep(3)\n",
    "print(\"STORE PAGE SCRAPE COMPLETED!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e168003-9e68-4ffb-bec3-10780c344e82",
   "metadata": {},
   "source": [
    "#### Storing the dictionary list\n",
    "\n",
    "Uncomment the below code if you wish to store the list of dictionaries as is to a file. \n",
    "\n",
    "The step is not necessarily needed as we will save the data to a csv file in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d0ee6-a6b8-4795-8471-f7683e56a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below code if you wish to pickle data in file\n",
    "\n",
    "\n",
    "#with open('target_store_data.txt','wb') as file:\n",
    "#    pickle.dump(target_data,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d67e1-de4d-4eae-aad2-ebf92053e538",
   "metadata": {},
   "source": [
    "#### Saving the Data!! <a class=\"anchor\" id=\"stda\"></a>\n",
    "\n",
    "Final step! We save the data to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66159038-241c-4fa2-95ce-65bcda9a183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data to csv file\n",
    "\n",
    "header =list(target_data[0].keys())\n",
    "\n",
    "with open('target_stores.csv','w',newline='') as csv_file:\n",
    "    w = csv.DictWriter(csv_file, fieldnames=header)\n",
    "    w.writeheader()\n",
    "    w.writerows(target_data)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
